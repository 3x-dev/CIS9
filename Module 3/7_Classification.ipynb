{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIS 9\n",
    "\n",
    "## Supervised Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading\n",
    "- Python Data Handbook Chapter 5: Decision Trees\n",
    "- Python Data Handbook Chapter 5: Gaussian Naive Bayes\n",
    "- Think Stats: Probability - Bayes's Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, or classification predictive modeling, uses algorithms that learn from a set of input features X and associated labels y, such that when presented with a new set of features X, the algorithms can correctly predict the y outcome. On the surface, classification sounds like regression: using training data X and y to learn, then correctly predict the output y for some new X. And that's because both classification and regression are supervised learning.\n",
    "\n",
    "The difference between regression and classification is in the type of X and y data.\n",
    "- For regression, the X and predicted y data are _quantitative_ data, or a number within a range of values. An example of quantitative data is the cost of a car within a range of 0 to 200,000 dollars.\n",
    "- For classification, the X and predicted y data are _categorical_ data, or a type of object from a set of different types. An example of categorical data is the type of car from the categories of compact, sport, sedan, or luxury cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "A decision tree is one of the most common _classification_ model, but as we've seen in the previous discussion on regression ML models, a decision tree can also be used for a _regression_ problem.<br>\n",
    "Here we explore in more detail how a decision tree works as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data in the file `zoo.csv` ([source](https://www.kaggle.com/uciml/zoo-animal-classification)). The dataset consists of features of different animals at a zoo, and the animals are divided into 7 different types: 4-legged animals, fish, birds, crustaceans, amphibians, insects, reptiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read data into a DataFrame called __a__. Then print the size and first 5 lines of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the DataFrame: (101, 18)\n",
      "  animal_name  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
      "0    aardvark     1         0     0     1         0        0         1   \n",
      "1    antelope     1         0     0     1         0        0         0   \n",
      "2        bass     0         0     1     0         0        1         1   \n",
      "3        bear     1         0     0     1         0        0         1   \n",
      "4        boar     1         0     0     1         0        0         1   \n",
      "\n",
      "   toothed  backbone  breathes  venomous  fins  legs  tail  domestic  catsize  \\\n",
      "0        1         1         1         0     0     4     0         0        1   \n",
      "1        1         1         1         0     0     4     1         0        1   \n",
      "2        1         1         0         0     1     0     1         0        0   \n",
      "3        1         1         1         0     0     4     0         0        1   \n",
      "4        1         1         1         0     0     4     1         0        1   \n",
      "\n",
      "   class_type  \n",
      "0           1  \n",
      "1           1  \n",
      "2           4  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "a = pd.read_csv('zoo.csv')\n",
    "\n",
    "# Print the size of the DataFrame\n",
    "print(\"Size of the DataFrame:\", a.shape)\n",
    "\n",
    "# Print the first 5 lines of the DataFrame\n",
    "print(a.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are indeed 7 different class_type values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the X data, which is the DataFrame a but with no 'animal_name' or 'class_type' columns.\n",
    "<br>Create the y data, which is the 'class_type' column.\n",
    "<br>Print the size of X and y, and show the first 5 lines of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (101, 16)\n",
      "Size of y: (101,)\n",
      "   hair  feathers  eggs  milk  airborne  aquatic  predator  toothed  backbone  \\\n",
      "0     1         0     0     1         0        0         1        1         1   \n",
      "1     1         0     0     1         0        0         0        1         1   \n",
      "2     0         0     1     0         0        1         1        1         1   \n",
      "3     1         0     0     1         0        0         1        1         1   \n",
      "4     1         0     0     1         0        0         1        1         1   \n",
      "\n",
      "   breathes  venomous  fins  legs  tail  domestic  catsize  \n",
      "0         1         0     0     4     0         0        1  \n",
      "1         1         0     0     4     1         0        1  \n",
      "2         0         0     1     0     1         0        0  \n",
      "3         1         0     0     4     0         0        1  \n",
      "4         1         0     0     4     1         0        1  \n"
     ]
    }
   ],
   "source": [
    "X = a.drop(['animal_name', 'class_type'], axis=1)\n",
    "\n",
    "y = a['class_type']\n",
    "\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Size of y:\", y.shape)\n",
    "\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Note that the data in x are conveniently numbers already. Otherwise we'd have to convert any text strings into numbers.\n",
    "<br>Split the data into a training set and a test set.\n",
    "<br>Show the size of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train: (70, 16)\n",
      "Size of X_test: (31, 16)\n",
      "Size of y_train: (70,)\n",
      "Size of y_test: (31,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print(\"Size of X_train:\", X_train.shape)\n",
    "print(\"Size of X_test:\", X_test.shape)\n",
    "print(\"Size of y_train:\", y_train.shape)\n",
    "print(\"Size of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "regr = DecisionTreeClassifier()\n",
    "regr = regr.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = regr.score(X_train, y_train)\n",
    "test_accuracy = regr.score(X_test, y_test)\n",
    "\n",
    "print(\"Training set accuracy:\", train_accuracy)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inspect the decision tree that was created by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8,7)\n",
    "tree.plot_tree(regr, fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. See how well the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Since the dataset is small, we create a DataFrame to visually compare each actual\n",
    "# and predicted y value\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, it's not practical to inspect each y output so we query the model for the accuracy score instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see how well the model works is with the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred, labels=np.arange(1,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are the predicted values (or y_pred) and the rows are the actual values (or y_test).\n",
    "<br>With a 100% accurate score, the matrix will have non-zeros only across the diagonal from [0,0] to [n,n].\n",
    "<br>If there are values that are not on the diagonal, it means the model made some wrong predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with multi-linear regression, the model can also tell us the contribution of each X feature in predicting the y output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(regr.feature_importances_, X.columns, columns=['Importance'])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending from the Decision Tree model is the __Random Forest__ model. For some applications one decision tree is not sufficient to produce effective output. Random Forest is an algorithm that uses multiple decision trees (or a forest of trees) to make decisions. The algorithm randomly creates decision trees, and each node in a decision trees has a random subset of features to calculate the output. The algorithm then combines the output of the individual decision trees to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor\n",
    "\n",
    "The K Nearest Neighbor, or KNN, algorithm is called a lazy learning algorithm, which means it does not use any training data to generate the model. This is in contrast to the Decision Tree algorithm, which generates the tree during training. For KNN, all training data are only stored but not used during the training stage. During the testing stage the algorithm will use the training data with the test data to make the predictions. This makes training faster but makes the testing slower.\n",
    "\n",
    "To predict an outcome, the algorithm plots the new data with the stored training data. Then it locates the training data points that are closest to the new data. These nearest training data points, or the nearest neighbors, determine the output prediction of the new data. For example, if the features of the new data cause it to be plotted near training data of type T, then the prediction for the new data will be type T.\n",
    "\n",
    "In KNN, K is the number of nearest neighbors that the algorithm will use to make the prediction. We can adjust the K value to help the model be more accurate. A small K value may not be as accurate as a larger K value, but a K value that's too large will include neighbors that are too far away and introduce errors into the evaluation or be computationally costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We use the same data from `zoo.csv` above, so we can still use the same training and testing data.\n",
    "<br>Create a KNN classifer, train and test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Check how well the classifier works by doing the same comparison as step 6.<br>\n",
    "Then find the accuracy score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of actual and predicted y values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the accuracy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear regression and decision tree models, we've been using default parameters for the models. But for KNN there is a parameter K that requires an input. The K value is a way that we can do some basic _tuning_ of the algorithm to make it more accurate. \n",
    "\n",
    "We can loop through the K values to observe the accuracy of the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we loop through K values from 1 to 50, using the same X and y datasets\n",
    "errors = []\n",
    "for i in np.arange(1, 50):\n",
    "    c = KNeighborsClassifier(n_neighbors = i)\n",
    "    c.fit(X_train, y_train)\n",
    "    y_pred = c.predict(X_test)\n",
    "    errors.append(np.mean(y_pred != y_test))\n",
    "    \n",
    "# see what the error rate looks like\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.grid(True)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can see that the prediction is most accurate when K is 2. We also see that as we use a higher number of neighbors, or higher K, the error rate goes up. This means that looking at many of the closest neighbors doesn't help the model to arrive at a good prediction for what type of animal it is. \n",
    "\n",
    "Rebuilding the model with K=2 to show that we have slightly better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "The Naive Bayes (NB) model is based on the Bayesian theorem, which calculates the likelihood of an output label, given some observed features. For example, if the features are dark clouds and strong winds, then the likelihood for stormy weather is high and the likelihood for sunny weather is low.\n",
    "\n",
    "When calculating the likelihood of the output label, the model assumes that all features are independent of each other, which isn't always true, and this is the reason why the model is called naive. \n",
    "\n",
    "We start by investigating the Gaussian Naive Bayes model. The Gaussian term means that we assume the features have a Gaussian distribution, which is a common distribution for many events or measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We start by observing the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many features are independent, such as being aquatic and being a predator, but some features are dependent, such as having hair means not having feathers. \n",
    "<br>With the NB model, if an animal has hair, is a predator with teeth, backbone, and other features of a bear, then the probability of it being a bear is high, and the model will classify this animal as a bear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Use the same training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Create the classifier, train the classifier, test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Check the result by comparing actual and predicted data, then show the accuracy score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of Naive Bayes classification is the __Multinomial Naive Bayes__ model. The model also uses the combined probabilities of the features to predict the output, but it also takes into account the number of occurrences of a feature. \n",
    "\n",
    "Multinomial Naive Bayes is often used for classification of text or reading material. For example, if a reading passage has the words 'Congress', 'election', 'vote', 'political', '2020', then there's a small chance of it being about the 2020 election, but if the '2020' appears many times in the reading, then the probability of it being about the 2020 election is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring classifications\n",
    "<br>In addition to the accuracy score, another common measurement for classification problems is the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the rate of correct predictions. It is used when we want to know the number of correct predictions, which is when the algorithm correctly predicts a type T when it is actually type T.\n",
    "\n",
    "Precision is the ratio of correct predictions for type T / total number of predictions for T\n",
    "<br>Recall is the ratio of correct predictions for type T / all actual type T\n",
    "<br>The F1 score is a type of mean between precision and recall. It is used when the datasets don't have an equal representation for each type that's being classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discussed 3 of the common classification models in supervised learning and how they work in general. It takes some more in-depth knowledge and statistical background to _tune_ or adjust a model so that it can be more accurate with a particular application. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
